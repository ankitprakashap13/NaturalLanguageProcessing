{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1633858740956,"user":{"displayName":"Kavitha Chetana Didugu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR9fFlhcAForHB-O9VkrzPpHu-cpY20ZjiXXLIyU8=s64","userId":"07214785061351746681"},"user_tz":-330},"id":"zhXsYJwq7-Rs","outputId":"858a8758-8557-4000-ec4a-209eb497680e"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'gensim'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[0;32m     18\u001b[0m \u001b[39m#from gensim.models.doc2vec import Doc2Vec, TaggedDocument\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"]}],"source":["# Importing required libraries\n","\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import nltk\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer\n","import numpy as np\n","import re\n","\n","from gensim.models import Word2Vec\n","#from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.manifold import TSNE\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# nltk downloaded (run only once)\n","nltk.download('stopwords',quiet=True) # stopword library\n","nltk.download('wordnet', quiet=True) # wordnet library\n","nltk.download('words', quiet=True) # words library\n","nltk.download('punkt', quiet=True) # tokenize library\n"]},{"cell_type":"markdown","metadata":{"id":"KNmyqGakl-iU"},"source":["### Load the dataset from the disk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1633858740962,"user":{"displayName":"Kavitha Chetana Didugu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR9fFlhcAForHB-O9VkrzPpHu-cpY20ZjiXXLIyU8=s64","userId":"07214785061351746681"},"user_tz":-330},"id":"s_Bu4lfx7-Rz","outputId":"d61eb275-40ec-4760-d786-fe5fbf8ed508"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv('bbc-text.csv')\n","print(df.head())\n","print('-'*60)\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":135},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1633858740963,"user":{"displayName":"Kavitha Chetana Didugu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR9fFlhcAForHB-O9VkrzPpHu-cpY20ZjiXXLIyU8=s64","userId":"07214785061351746681"},"user_tz":-330},"id":"v0wflxvWOvut","outputId":"0a82b5af-4dda-45cc-e75b-bc8cccbc0c4d"},"outputs":[],"source":["df['text'][0]"]},{"cell_type":"markdown","metadata":{"id":"UOrEAKBg2Gmq"},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByDnzsBJ2Gmq"},"outputs":[],"source":["## Preprocessing defenitions\n","\n","def remove_punctuation(text):\n","    return re.sub('[^a-zA-Z]', ' ', str(text))\n","\n","def lower_case(text):\n","    return text.lower()    \n","\n","def remove_tags(text):    \n","    return re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", text)\n","\n","def remove_special_chars_and_digits(text):\n","    return re.sub(\"(\\\\d|\\\\W)+\",\" \", text)\n","\n","#def remove_stop_words(tokenized_text):\n","#    return [w for w in tokenized_text if not w in set(stopwords.words('english'))]\n","\n","def stopword_lemma(text):\n","    token = nltk.word_tokenize(text)\n","    text_stop = [x for x in token if x not in set(stopwords.words('english'))]\n","    lemmatizer = WordNetLemmatizer()\n","    text_lemma = [lemmatizer.lemmatize(word) for word in text_stop]\n","    text_lemma = ' '.join(text_lemma)\n","    return text_lemma\n","\n","\n","def normalize_text(text: str) -> str:\n","    text = remove_special_chars_and_digits(text)\n","    text = remove_punctuation(text)\n","    text = remove_tags(text)\n","    text = lower_case(text)\n","    text = stopword_lemma(text)\n","    \n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDA-oHn42Gmr","outputId":"94327f8d-03c0-400e-9df8-391d358623d9"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGKRxHgu2Gms","outputId":"4e3ed330-6ede-4107-f51f-ee19ddba14ad"},"outputs":[],"source":["# Creating a new feature with normalized text\n","\n","df['normalized_text'] = df['text'].apply(normalize_text)\n","df.loc[:, ['text', 'normalized_text']].head()"]},{"cell_type":"markdown","metadata":{"id":"amXmuUVT2Gmt"},"source":["### Vectorizing Documents\n","\n","### TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_VXDvMm2Gmt","outputId":"a299da21-ba21-4d2b-858f-3e8d2ef4fff7"},"outputs":[],"source":["vectorizer_tfidf = TfidfVectorizer(stop_words=set(stopwords.words('english')))\n","tfidf_corpus = vectorizer_tfidf.fit_transform(df['normalized_text'])\n","tfidf_corpus.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDL4QCAE2Gmt","outputId":"e253b51d-8134-44e5-a7fb-42a54b57f9f8"},"outputs":[],"source":["# Converting the tf-idf corpus to dataframe\n","\n","tfidf_vectors_corpus = pd.DataFrame(tfidf_corpus.toarray(), \n","                                    columns=vectorizer_tfidf.get_feature_names(), \n","                                    index=df.index)\n","tfidf_vectors_corpus.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uGqpwwX2Gmu","outputId":"4ab32f72-e26a-4d71-b2a7-9dc9c876c31d"},"outputs":[],"source":["tfidf_vectors_corpus.loc[0, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GOdI4lB2Gmu","outputId":"8e6a4b8b-b938-49e7-8827-3d8f72673ccb"},"outputs":[],"source":["tfidf_vectors_corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GONfq4Eg2Gmu","outputId":"9c089af1-32f6-4d59-9857-d5f405c85b52"},"outputs":[],"source":["# Calculate tfidf for all columns and list top 10\n","tfidf_vectors_corpus.mean().sort_values(ascending = False).head(10)"]},{"cell_type":"markdown","metadata":{"id":"ttQBnVLK2Gmv"},"source":["### Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDfQ0ZyP2Gmv","outputId":"8047f1a9-1450-418f-e11a-d21603d5975d"},"outputs":[],"source":["# You may take a subset of 50 tokens for this exercise. These 50 may be ‘random’ or top 50 tokens with the highest tf-idf scores.\n","top50_tokens_tfidf = (tfidf_vectors_corpus.mean()\n","                                          .sort_values(ascending=False)\n","                                          .head(50)\n","                                          .index)\n","print(top50_tokens_tfidf.shape)\n","top50_tokens_tfidf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gB5NBDSl2Gmw","outputId":"a747f3c5-97a1-4204-f9e3-4b60b9a14741"},"outputs":[],"source":["df.tokenized_text = df.normalized_text.apply(lambda x : nltk.word_tokenize(x))\n","model_word2vec = Word2Vec(sentences=df.tokenized_text,  #Default - CBOW model, to get skip gram set sg=1\n","                          size=300,  #Embedding size = 300 --> Change as per the need  \n","                          min_count=1)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ff1irYRp2Gmw","outputId":"4c264e54-6dec-4240-d974-1a608309b256"},"outputs":[],"source":["model_word2vec.wv['people']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77bri4zH2Gmw","outputId":"0f62a341-f431-4e95-eada-065fd8d7d373"},"outputs":[],"source":["model_word2vec.wv.most_similar('people')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuZlkoiL2Gmw"},"outputs":[],"source":["# Extracting the vectors\n","\n","top50_tokens_tfidf_vectors = {token: model_word2vec.wv[token] \n","                               for token in top50_tokens_tfidf}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bq4kcSdK2Gmx","outputId":"3e47c75b-3b7d-471b-eefc-ca1405f8bd6c"},"outputs":[],"source":["# Converting it to dataframe\n","\n","top50_tfidf_word_vectors_df = (pd.DataFrame(top50_tokens_tfidf_vectors)\n","                               .transpose())\n","top50_tfidf_word_vectors_df"]},{"cell_type":"markdown","metadata":{"id":"mNzFRQ872Gmx"},"source":["- Change the embedding size and check the preformance\n","- Once word vectors are generated, we can then compute the cosine similarity between each pair of word vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2tY_69y2Gmx"},"outputs":[],"source":["# Computing the similarities for for embedding size 300\n","similarities = cosine_similarity(top50_tfidf_word_vectors_df, \n","                                 top50_tfidf_word_vectors_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEV-yADA2Gmx","outputId":"a7111c72-06a7-4bf8-ddb5-e7848edbeecd"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(20, 20))\n","sns.heatmap(ax=ax, \n","            data=similarities,\n","            xticklabels=top50_tfidf_word_vectors_df.index, \n","            yticklabels=top50_tfidf_word_vectors_df.index)"]},{"cell_type":"markdown","metadata":{"id":"VRPMNckl2Gmx"},"source":["- The similarity is done for embedding size of 300 and top 50 words from TF-IDF. "]},{"cell_type":"markdown","metadata":{"id":"XJq6cYeP2Gmy"},"source":["### T-SNE Plot to show the similarities between words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrqFB1Uc2Gmy"},"outputs":[],"source":["tsne_model = TSNE(n_components=2, random_state=32)\n","new_values = tsne_model.fit_transform(top50_tfidf_word_vectors_df) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiIkAYwo2Gmy","outputId":"6cd2355d-840d-4908-a2e0-6412fcf57a72"},"outputs":[],"source":["x = []\n","y = []\n","for value in new_values:\n","    x.append(value[0])\n","    y.append(value[1])\n","        \n","plt.figure(figsize=(20, 20))\n","labels = list(df.category)\n","\n","for i in range(len(x)):\n","    new_value = new_values[i]\n","    x = new_value[0]\n","    y = new_value[1]\n","        \n","    plt.scatter(x, y)\n","    plt.annotate(labels[i],\n","                    xy=(x, y),\n","                    xytext=(5, 2),\n","                    textcoords='offset points',\n","                    ha='right',\n","                    va='bottom')\n","    #plt.savefig(f'figures/{experiment_name}_tsne.png')\n","plt.show()\n","plt.close()"]},{"cell_type":"markdown","metadata":{"id":"sX_idyEj2Gmy"},"source":["# Glove Embeddings\n","- Using pre-trained glove model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXxtryRI2Gmy"},"outputs":[],"source":["# Importing the glove model and creating the embeddings\n","\n","import os\n","glove_path = 'glove.6B.300d.txt'\n","embeddings_index = {}\n","f = open(glove_path, encoding='utf8')\n","\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","    #print(embeddings_index)\n","f.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vd99j7vr2Gmy"},"outputs":[],"source":["# Verifying similar words from the pretrained embeddings\n","\n","from scipy import spatial\n","def find_similar_word(emmbedes):\n","  nearest = sorted(embeddings_index.keys(), key=lambda word: spatial.distance.euclidean(embeddings_index[word], emmbedes))\n","  return nearest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TChShyuR2Gmz","outputId":"fce2a4c9-babd-4fa0-9d02-098e7e833cee"},"outputs":[],"source":["find_similar_word(embeddings_index['people'])[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MALxtV_A2Gmz"},"outputs":[],"source":["# Convering the embedding words to vectors\n","\n","words = list(embeddings_index.keys())\n","vectors = [embeddings_index[word] for word in words]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vv3GIf3p2Gmz","outputId":"caf2dbd5-54c2-4857-acc9-2717f210bb4c"},"outputs":[],"source":["# Converting the vectors to dataframe\n","\n","embed_df =  (pd.DataFrame(vectors, index= words).transpose())\n","embed_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0sDK5yh2Gmz","outputId":"222c85fe-4cad-4aed-ce94-f474c4374297"},"outputs":[],"source":["# Extracting the top50 words\n","\n","top50_embed_vectors = {token: embed_df[token] \n","                               for token in top50_tokens_tfidf}\n","\n","top50_embed_vectors_df = (pd.DataFrame(top50_embed_vectors).transpose())\n","top50_embed_vectors_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qfdo0SDK2Gmz","outputId":"eff837bb-0a85-4f06-d95a-f90a312e3fff"},"outputs":[],"source":["# Cosine Similarity\n","# Computing the similarities for for embedding size 300\n","similarities = cosine_similarity(top50_embed_vectors_df, \n","                                 top50_embed_vectors_df)\n","\n","fig, ax = plt.subplots(figsize=(20, 20))\n","sns.heatmap(ax=ax, \n","            data=similarities,\n","            xticklabels=top50_embed_vectors_df.index, \n","            yticklabels=top50_embed_vectors_df.index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rA1Rmmai2Gmz","outputId":"9699a3e1-462e-42ba-b237-a6b0ee88bb4a"},"outputs":[],"source":["# TSNE Plot\n","\n","tsne_model = TSNE(n_components=2, random_state=32)\n","new_value = tsne_model.fit_transform(top50_embed_vectors_df)\n","\n","x = []\n","y = []\n","for value in new_values:\n","    x.append(value[0])\n","    y.append(value[1])\n","        \n","plt.figure(figsize=(20, 20))\n","labels = list(df.topic)\n","\n","for i in range(len(x)):\n","    new_value = new_values[i]\n","    x = new_value[0]\n","    y = new_value[1]\n","        \n","    plt.scatter(x, y)\n","    plt.annotate(labels[i],\n","                    xy=(x, y),\n","                    xytext=(5, 2),\n","                    textcoords='offset points',\n","                    ha='right',\n","                    va='bottom')\n","    #plt.savefig(f'figures/{experiment_name}_tsne.png')\n","plt.show()\n","plt.close()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OYZ703hVm5Cg"},"source":["Converting categrical labels to numerical format and further one hot encoding on the numerical labels."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1633858763119,"user":{"displayName":"Kavitha Chetana Didugu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR9fFlhcAForHB-O9VkrzPpHu-cpY20ZjiXXLIyU8=s64","userId":"07214785061351746681"},"user_tz":-330},"id":"tkOicZ3XkZq_","outputId":"5a9833af-add0-49d9-e028-3f8714ef3ffc"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":552},"executionInfo":{"elapsed":45,"status":"error","timestamp":1633858763127,"user":{"displayName":"Kavitha Chetana Didugu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR9fFlhcAForHB-O9VkrzPpHu-cpY20ZjiXXLIyU8=s64","userId":"07214785061351746681"},"user_tz":-330},"id":"EoR79rtZ7-Sr","outputId":"117ac1ae-0d5b-4bf1-afa9-595f892af739"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","df['Target'] = le.fit_transform(df['topic'])\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"DwAqGVql2Gm0"},"source":["### Train the classifier with Glove embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqDkNH382Gm0","outputId":"90d0b816-2224-459c-b00b-559aaece2ada"},"outputs":[],"source":["# Taking average of all word embeddings in a sentence to generate the sentence representation.\n","data_list = list()\n","for comp in df['text']:\n","    sentence = np.zeros(300)\n","    count = 0\n","    for w in normalize_text(comp):\n","        try:\n","            sentence += embeddings_index[w]\n","            count += 1\n","        except KeyError:\n","            continue\n","    data_list.append(sentence / count)\n","\n","len(data_list[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJtyXYgL2Gm0","outputId":"377a7f82-9477-4847-f568-73dae41e7423"},"outputs":[],"source":["## Train_Test split\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(np.array(data_list), df.Target.values, test_size=0.15, random_state=42)\n","print(X_train.shape, y_train.shape)\n","\n","### Training and Testing the classifier\n","\n","## Bernoulli model\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.metrics import accuracy_score\n","model_gloveembed_bnb = BernoulliNB()\n","model_gloveembed_bnb.fit(X_train, y_train)\n","pred = model_gloveembed_bnb.predict(X_test)\n","print('NB_Score:', accuracy_score(y_test, pred))\n","\n","## RandomForest model\n","from sklearn.ensemble import RandomForestClassifier\n","model_gloveembed_rf=RandomForestClassifier()\n","model_gloveembed_rf.fit(X_train, y_train)\n","pred = model_gloveembed_rf.predict(X_test)\n","print('RF_Score:', accuracy_score(y_test, pred))"]},{"cell_type":"markdown","metadata":{"id":"bhU5OMui2Gm0"},"source":["### Training the classifer with Word2Vec embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_5jbmVE2Gm1"},"outputs":[],"source":["## Taking average of all word embeddings in a sentence to generate the sentence representation.\n","data_list_wv = list()\n","for comp in df['text']:\n","    sentence = np.zeros(300)\n","    count = 0\n","    for w in normalize_text(comp):\n","        try:\n","            sentence += model_word2vec.wv[w]\n","            count += 1\n","        except KeyError:\n","            continue\n","    data_list_wv.append(sentence / count)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":34,"status":"aborted","timestamp":1633858763125,"user":{"displayName":"Kavitha Chetana Didugu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR9fFlhcAForHB-O9VkrzPpHu-cpY20ZjiXXLIyU8=s64","userId":"07214785061351746681"},"user_tz":-330},"id":"aQwfI-8uXIIy","outputId":"7efbe507-2140-41cc-c4bb-4ec8caf8245b"},"outputs":[],"source":["## Train_Test split\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(np.array(data_list_wv), df.Target.values, test_size=0.15, random_state=42)\n","#print(X_train.shape, y_train.shape)\n","\n","### Training and Testing the classifier\n","\n","## Bernoulli model\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.metrics import accuracy_score\n","model_w2v_bnb = BernoulliNB()\n","model_w2v_bnb.fit(X_train, y_train)\n","pred = model_w2v_bnb.predict(X_test)\n","print('NB_Score:', accuracy_score(y_test, pred))\n","\n","## RandomForest model\n","from sklearn.ensemble import RandomForestClassifier\n","model_w2v_rf = RandomForestClassifier()\n","model_w2v_rf.fit(X_train, y_train)\n","pred = model_w2v_rf.predict(X_test)\n","print('RF_Score:', accuracy_score(y_test, pred))"]},{"cell_type":"markdown","metadata":{"id":"ZCCpCyhw2Gm1"},"source":["#### Results clearly potraits that Glove models performs better than Word2Vec for the given dataset"]}],"metadata":{"anaconda-cloud":{},"colab":{"collapsed_sections":[],"name":"case_study.ipynb","provenance":[]},"kernelspec":{"display_name":"tf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"vscode":{"interpreter":{"hash":"78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"}}},"nbformat":4,"nbformat_minor":0}
